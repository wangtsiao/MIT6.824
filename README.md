# MIT6.824

### 1. MapReduce

MapReduce是非常有影响力的一篇论文，发表于2004年，它第一次向人们传达了计算可扩展性这个概念，即通过增加机器来处理大规模数据的任务。具体而言，一次任务在MapReduce中的处理流程主要分成两个阶段：Map - Patition - Reduce。下面通过两个例子简单传达MapReduce的技术思想。

- 对一个大文件中关键词计数。首先把大文件分割，之后多个Map进程并行的在每个文件中，按照关键词的哈希将其分到不同位置，之后Reduce对其进行处理，合并得到最终结果。
- 全排序。要把一亿个处于$[0,2^{64})$区间的数字按照大小排序，多个Map进程并行的将数字按照大小Patition到不同的位置，例如$[0,2^{16})$、$[2^{16},2^{17})$、对应不同的Reduce任务，之后每个Reduce将它们排序，最后合并起来得到结果。

在上面两个例子中，如果修改一部分数据，哪怕一点点数据，都需要重新执行整个MapReduce过程来更新结果，处理搜索引擎爬虫的增量更新，不免捉襟见肘。后面会介绍Percolator，是Google的一个增量更新的数据库。

### 2. Google File System





### 3. BigTable






### 4. Fault-Tolerance Virtual Machine

VMware公司在2010年发表的企业级（enterprise-grade）容错虚拟机。

使用主从节点的思想，即把单台机器上执行的虚拟机状态复制到备份机器上。主要亮点，对性能没有太大的影响（不到10%），对主从机器之间的带宽要求不高（20MB/s）。

The primary/backup is a common approach to implementing fault-tolerant servers, where a backup server is always avaiable to take over if the primary server fails.

为了随时应对当前机器失效的情况，因此备份机器的状态在任何时候都需要与当前机器保持一致（nearly identical）。一种直观的方法是把对当前机器状态的虽有改动都复制到备份机器上，包括CPU、内存、IO，然而这种方法需要发送大量的数据，尤其是内存改动设涉及到很多数据，因此对带宽的要求十分高，不能实际使用。

更少占用带宽的另外一种方法是把虚拟机抽象成状态机，准确的说是确切（deterministic）的状态机（statemachine）。让当前机器和备份机器从相同的初始状态开始，接收相同的操作序列，其最终状态就是相同的。然而问题在于，虚拟机上大多数操作并不是deterministic的，因此就需要一些额外的协调（coordination）机制，当然额外的信息没有占用太多的代快，从量上来看起码大大少于内存的状态。

把虚拟机看作是抽象的状态机，这个状态机上的操作就是虚拟机（包括所有设备）上的操作。与物理机一样，虚拟机上面也有non-deterministic的操作，例如时间读取、中断分配等，因此需要向备份机器发送额外的信息以保持同步。虚拟机运行在hypervisor上，这给上述状态机的方案提供了很好的条件。好处就在于hypervisor拥有对虚拟机的执行控制权，包括所有输入信息，还有所有的关于non-deterministic信息，因此就可以在备份机器上重做这些操作。

这个虚拟机可以在当前机器失效后，立马切换到备份机器，然后继续往新的备份机器复制数据。当然，也存在一些问题，比如这些机器的处理器都是单核处理器，因为多核处理器带来一些共享内存的non-deterministic操作，因此带来很大的性能影响，这个工作仍在进展中。

对于non-deterministic操作，发送足够的信息使其成为deterministic操作，即使其有相同的状态改变和输出信息。对于non-deterministic事件，例如IO中断，发送事件发生时所处代码流的位置，备份机器上重做时在相同的位置执行代码流。

正式的说明容错需要达到的目的，即Output Requirement：如果备份虚拟机在主虚拟机发生故障后接管，则备份虚拟机将继续以与主虚拟机发送到外部世界的所有输出完全一致的方式继续执行。即Client对于这种故障切换是没有感知的。这非常简单，只需要当前机器把所有的输出延迟，只有当备份机器收到输出操作前的所有信息后，在输出给Client。然而，试想当前机器在执行完输出操作后故障，那么备份节点就需要知道它该重做日志，然后上线代替故障机器，此时就会出现一种情况，备份节点在执行那个输出操作之前，一些non-deterministic事件发生，例如中断/计时器等加入，就会被改变执行流进而不会执行上述输出操作。

备份机器和当前机器共享同一个硬盘，当发生网络分区时，两个机器不能相互通信，备份机器收不到对方的心跳包就要上线，但此时当前机器其实仍然处于上线状态，如果两者同时上线运行一定会出现问题，因此需要一定的机制避免这种情况。当备份机器上线之前需要test-and-set检测共享磁盘中的变量，如果不满足条件则持续等待。

当前机器与备份机器之间的Log通道以缓冲区的形式维护，如果备份机器重做的速度低于当前机器，那么Log通道一定会逐渐变满，当满了之后当前机器会被迫停止，等到有缓冲区时继续执行。此外，为了防止备份机器因为各种原因落后当前机器太多，会主动的调低当前机器的CPU频率来同步两个机器，当然这种情况并不是很多，不然会带来太大的性能损耗。

还要考虑到对当前机器的人为操作，例如关机，或者增加CPU频率等。也要通知到备份机器。

3.4和3.5章节的磁盘与网络IO的处理，没有细看。

接下来介绍了两个可选的其他实现方案。第一个是共享磁盘与非共享磁盘。对于非共享磁盘，当前机器对磁盘的写操作不必等到备份机器重做确认后再执行，并且也防止了共享磁盘故障后两者都不可恢复的情况。当然，也存在缺点，非共享磁盘必须通过某种方式同步磁盘，而且处理上述test-and-set操作时，还需要第三方服务器支持。第二个是备份机器是否从磁盘读数据，论文的方案是不从磁盘读数据，只重做Log通道的信息，从磁盘读数据一种显而易见的优势是可以减少带宽需求，但缺点是带来了性能损耗，例如备份机器需要等到读完数据才能往下执行，如果读磁盘失败还需要一种重复尝试读取。

最后是未来可能的研究方向，可以看到现在的方案只支持单核处理器，虽然作者声称单核处理器对于大多数工作场景已经绰绰有余，但多核处理器明显是接下来要做的，一种最简单的方法是横向扩展，即使用多个单核VM来处理任务。当前来看，高性能的多核处理器重做是一个活跃的研究领域。


### 5. Percolator

大家好，我们组的分享是Google在2010年发表在OSDI会议上的一项工作，关注于大规模数据的增量处理。论文的题目也非常简洁明了，使用分布式事务和通知来支持增量处理。


这其实是一个比较大的话题，搜索引擎业务一方面涉及的数据量很大，要求能高效计算；另一方面机器总会因为各种原因故障，要求有容错机制。所以早年间Google也在想方法高效并且可靠的处理大规模数据。我们看到Google在03年、04年、06年分别发表了G File System、MapReduce、Bigtable这三篇非常有影响力的论文，后续很多推动业界进展的开源项目都或多或少借鉴了这三篇论文中的思想。



今天我们组分享的这篇论文，算是Google在当时背景下承前启后的工作，一方面在Bigtable基础上提供了分布式事务，用增量处理替代了MapReduce这种批处理的方式；另一方面开启了后续Spanner、F这些分布式数据库工作。不过，虽然这是一项带有过渡性质的工作，但是这篇论文中提出的基于二级锁的分布式事务两阶段提交方法，现在被很多国产开源数据库采用，接下来我们会看到。



那按照惯例，首先交代下问题背景。搜索引擎每隔一段时间要爬取全世界所有网站的数据，这是一个数据量巨大的工作。如果按照链接引用关系，爬取的初始数据是一张图，这种非结构化数据需要处理成倒排索引的数据结构，才可以服务于搜索引擎，比如计算PageRank值、维护关键词到链接的映射关系等等。



MapReduce是一个非常有影响力的工作，它第一次向业界传达可扩展计算这个概念。Google也的确在很长一段时间里用这种方式来批量处理任务。下面有一张图，简洁的说明了这种方法的思想，借助于G File System这种分布式存储引擎，MapReduce有一个master节点负责协调调度，有若干个map任务并行处理数据，然后利用哈希函数/或者是自定义函数，把结果写入若干个不同文件，在所有map任务完成之后，接着若干个reduce任务并行的处理map之后的数据。不再举具体的例子了，因为现在MapReduce的编程范式已经十分成熟。



说一下这种方式在处理网页数据时的不足之处，假如现在已经有了一些数据，通过MapReduce的方式把它处理成目标格式，然后过了一段时间，又爬取了一些数据，就需要再次通过MapReduce的方式把它们一起处理。显然这种方式承受了重复计算的负担，一方面，新爬取的数据和已有的数据相比，有很多是重复的；另外一方面，每次新爬取数据，就需要处理全部的数据，效率低下而且浪费资源。作者希望做一个能支持增量处理大规模数据的，并且能够让网页索引更新非常快的系统，比方说网站一旦更新或者上线，搜索引擎可以很快的把网站内容收录进来。这篇论文提出的系统叫做Percolator，它是基于Bigtable构建的系统。



Bigtable基于G File System的一个三维表，其中保存的每一个数据都由行键、列键以及时间戳唯一决定，直观来看就是字符串类型的行和列以及一个64比特整型的时间戳，对应一个字符串类型的值。下面是一个例子，行键是www/cnn/com，其中contents按照时间戳保存了三份，两个列表示有两个锚链接指向这个网站，相同前缀名称的多个列组成一个family，family是Bigtable中访问控制、使用磁盘或内存的基本单位，Bigtable可以以行为单位为读写操作提供原子性。



粗略的说，可以把Percolator看作是在Bigtable基础上添加了支持快照隔离的、跨行操作的、支持ACID的、分布式事务系统。为了直观传达Percolator事务的流程，下面给一个Bob向Joe转账7块钱的例子，这个表有两行三列，但其实每一行的属性只有一个，余额，这三列是一个family，第一列表示余额数据，第二列表示余额的锁，第三列表示余额的提交情况。在事务开始之前，客户端从时间戳服务器获取逻辑时间戳，这里是7，在对Bob的余额减7元之前，要先写入锁，这个第一个写入的锁被称为主锁，接下来事务中写入的锁都称为二级锁，然后对Alice的余额加7之前，要写入二级锁。最后是提交，提交前客户端要获取逻辑时间戳， 这里是8，先删除主锁，在提交列写入指向数据的逻辑时间戳，之后对Alice的数据执行同样的操作。这个例子说明了二级锁使用流程，算是为并发事务提供了基本工具。



Percolator为并发事务提供了快照隔离级别，是一种乐观的并发控制协议，可以看作是一种多版本协议。举一个例子，这张图中空心方形和实心圆形分别表示事务的开始时间戳和提交时间戳，事务2和事务1所看的数据是同一个版本，在每个事务提交时会检测是否跟其他事务有写写冲突，如果有，让那个开始时间戳早的事务通过。在Percolator中，因为有逻辑时间戳服务器，所以快照隔离级别不难实现。



因为是分布式的操作，任何一个机器都有可能发生故障，所以对于写事务使用两阶段提交协议。在Percolator中，客户端充当两阶段提交协议中的协调者，在第一阶段，按照快照隔离要求，如果当前事务看到有其他事务在它开始时间戳之后进行写操作，就放弃执行，如果看到这一列中有锁存在，也放弃执行。这里可能存在一种情况，其他事务在提交之后释放了锁，但是释放的比较慢，导致当前事务检测到这个残存的锁，作者认为这种情况虽然会发生，但是发生的概率极低，因此不做额外的考虑，直接放弃当前事务。上述两项检测通过后，把锁和要写的数据按照开始时间戳写到对应的位置。在第二阶段，先释放锁，然后写入提交，这里有一个约定，一旦主锁被释放，这个事务一定会被提交。



当然不可避免会发生机器故障，如果Bigtable的服务器故障，其实无需处理，因为Bigtable提供了容错机制，保证在机器故障情况下依然可以正常提供服务。相对而言，因为客户端在两阶段提交协议中充当协调者，所以对客户端故障的处理比较复杂一点。先说如果第一阶段故障，每个事务定期在chubby中写入一个时间戳来表示它在运行，chubby是Google的一个分布式锁服务，类似于G File System，不过好处是chubby支持小文件。当一个事务A遇到事务B的锁时，A通过chubby检查B是否在运行，如果发现B事务已经挂掉了，就清理它的锁，因为清理锁和提交事务都需要主锁，所以这两个行为只能成功一个，要么成功提交，要么被清理。接着说第二阶段故障，此时故障的事务B可能已经清理了主锁，事务A要决定把事务B前滚还是回滚，这取决于主锁是否还在，因为前边提到过一个约定，只要主锁被释放掉，就代表事务提交。所以如果主锁还在就回滚，如果主锁不在就前滚，来保障事务的原子性。



好了，以上就是percolator基于二级锁的两阶段提交协议，这个协议现在被开源数据库广泛使用，比如TiDB、OceanBase。



接下来简单说下这篇论文中的通知，因为percolator主要关注增量处理，它主要通过observer这种机制来实现，大伙儿可以把它简单理解成回调函数，每当被观察列的数据被修改，就执行这个函数。这个机制和数据的触发器很像，但是有一点不太一样，触发器是在事务内执行，而percolator的是另起一个事务执行。所以说percolator的开发者在进行代码开发时就要注意obverser环，就是修改了一个数据然后执行observer，接着observer又修改数据，接着有observer，循环往复的执行，对于这种情况percolator并没有提供保护机制。



好，接下来时间交给我的小伙伴儿。















